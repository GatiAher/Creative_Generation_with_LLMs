{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Querying Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import pandas\n",
    "\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# Add src module to path before import.\n",
    "sys.path.insert(0, str(pathlib.Path('../src')))\n",
    "\n",
    "from file_IO_handler import get_plaintext_file_contents\n",
    "from fill_string_template import get_filled_strings_from_dataframe, FilledString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Access to Replicate\n",
    "\n",
    "Following: [meta-llama/llama-recipes - Quickstart](https://github.com/meta-llama/llama-recipes/blob/main/recipes/quickstart/Getting_to_know_Llama.ipynb)\n",
    "\n",
    "We will use Replicate hosted cloud environment.  \n",
    "Obtain Replicate API key â†’ https://replicate.com/account/api-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICATE_API_KEY = get_plaintext_file_contents(pathlib.Path(\"../REPLICATE_API_KEY.env\"))\n",
    "# print(REPLICATE_API_KEY)\n",
    "\n",
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_KEY\n",
    "\n",
    "# use model hosted Replicate server for inferencing\n",
    "llama2_70b_chat = \"meta/llama-2-70b-chat:2d19859030ff705a87c746f7e96eea03aefb71f166725aee39692f1476566d48\"\n",
    "llama2_13b_chat = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use replicate's hosted api\n",
    "import replicate\n",
    "\n",
    "# text completion with input prompt\n",
    "def Completion(prompt):\n",
    "  output = replicate.run(\n",
    "      llama2_13b_chat,\n",
    "      input={\"prompt\": prompt, \"max_new_tokens\":1000}\n",
    "  )\n",
    "  return \"\".join(output)\n",
    "\n",
    "# chat completion with input prompt and system prompt\n",
    "def ChatCompletion(prompt, system_prompt=None):\n",
    "  output = replicate.run(\n",
    "    llama2_13b_chat,\n",
    "    input={\"system_prompt\": system_prompt,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_new_tokens\":1000}\n",
    "  )\n",
    "  return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Completion(prompt=\"The typical color of a llama is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh my gosh, you're so close! The typical color of a llama is... (drumroll please)... GRAY! es, llamas are known for their beautiful gray coats, which can range in shade from light to dark. Some llamas may even have white markings on their faces or legs, but overall, gray is the most common color you'll see. o you have any other questions about llamas?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ChatCompletion(\n",
    "    prompt=\"The typical color of a llama is: \",\n",
    "    system_prompt=\"respond with only one word\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Gray.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ChatCompletion(\n",
    "    prompt=\"The typical color of a llama is: \",\n",
    "    system_prompt=\"response in json format\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' {\\n\"answer\": \"The typical color of a llama is brown.\"\\n}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama_2_metaphor_generation_with_LLMs]",
   "language": "python",
   "name": "conda-env-llama_2_metaphor_generation_with_LLMs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
